{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** Write a python script for extracting the following attributes in a product url, for example https://www.chewy.com/pedigree-adult-complete-nutrition/dp/141438\n",
    " \n",
    "- ProductName and Description\n",
    "- Size\n",
    "- Ingredients\n",
    "- Images (all image location hrefs)\n",
    "- Categories\n",
    "- Key Benefits\n",
    "- Brand (Manufacture)\n",
    "- Price (for each packaging size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.1.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import requests      #send request to HTML page\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup     #python library for extracting data\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "import pandas as pd                #Further Analysis of the extracted Data\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the lists to store the extracted data\n",
    "# The data that we extract is unstructured data. So we’ll create empty lists to store them in a structured form,\n",
    "count=0                  # Intialize search row count\n",
    "products=[]              #List to store name of the product\n",
    "#prices=[]                #List to store price of the product\n",
    "ratings=[]               #List to store rating of the product\n",
    "#specifications = []     #List to store specifications of the product\n",
    "ProductName_Description = []                 #List to store ProductName_Description sspecifications of the product\n",
    "Size = []                 #List to store ssize specifications of the product\n",
    "Ingredients= []                  #List to store Ingredients specifications of the product\n",
    "Images = []                  #List to store images specifications of the product\n",
    "Categories = []             #List to store Categories specifications of the product\n",
    "Key_Benefits = []          #List to storeKey Benefits specifications of the product     \n",
    "Brand  = []               #List to brand specifications of the product \n",
    "price = []                #List to price Categories specifications of the product\n",
    "df=pd.DataFrame()        #Initialize Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fake_useragent.fake.FakeUserAgent object at 0x0000021FAFB84D08>\n"
     ]
    }
   ],
   "source": [
    "# Creating an User agent  pip insatll fake-useragent\n",
    "# A User agent acts as a bridge between the user and the internet . \n",
    "# It gives the webserver necessary information about our browser, software, device type and etc.\n",
    "# According to this information the web servers can display different webpages for us\n",
    "# The web server uses this information to adapt the content to specific web browsers and different foods specifications \n",
    "# https://pypi.org/project/fake-useragent/    \n",
    "# read here\n",
    " \n",
    "user_agent = UserAgent() # Dummy User Agent\n",
    "print(user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the product name. we are searching for laptops\n",
    "# The extracted data will be related to that product.\\ # Search for Laptops\n",
    "product_name = 'AdultFood'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.chewy.com/pedigree-adult-complete-nutrition/dp/141438\n"
     ]
    }
   ],
   "source": [
    " #Find Elements by ID\n",
    "#To extract data from multiple pages of the product listing we’re going to use a for loop.\n",
    "# The range will specify the number of pages to be extracted\n",
    "\n",
    "#url = \"https://www.chewy.com/search?q={0}&page={1}\" \n",
    "#print( url.format(products,1))           \n",
    "url=\"https://www.chewy.com/pedigree-adult-complete-nutrition/dp/141438\"\n",
    "print(url.format(product_name,1))    #run and check this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.chewy.com/pedigree-adult-complete-nutrition/dp/141438\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3): # Limiting search to 3 pages due to multiple redirection issues for higher number of pages\n",
    "    url = \"https://www.chewy.com/pedigree-adult-complete-nutrition/dp/141438\" # Scrape data from chewy.com\n",
    "    url = url.format(product_name,i)\n",
    "    print(url)\n",
    "    \n",
    "    ## Getting the reponse from the page using get method of requests module\n",
    "    #page= requests.get(url, headers ={\"User_agent\": user_agent.chrome})\n",
    "    page= requests.get(url)\n",
    "    print(page)\n",
    "    \n",
    "    ## Storing the content of the page in a variable\n",
    "    html=page.content\n",
    "    print(html)\n",
    "    \n",
    "    # To Extract data from html file --- Creating BeautifulSoup object\n",
    "    page_soup=bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    print(page_soup.prettify())     #will show as a nested html file\n",
    "    #it gives the visual representation of the parse tree created from the raw HTML content.\n",
    "\n",
    "    \n",
    "    #Iterate over page_soup.find_all('p')   \n",
    "    # this will iterate over all paras\n",
    "    print(page_soup.find_all('p')[0].get_text())\n",
    "\n",
    "    \n",
    "    ## Decoding the tags\n",
    "    #('a',{'class':'_cw-desktop'})\n",
    "\n",
    "    for containers in page_soup.findAll('a',{'class':'alt-image dark-bg no-expand-thumbnails mz-thumb'}): \n",
    "       # <a href=\"#search-autocomplete\" class=\"sfw-header__skip-link\">Skip to search</a>\n",
    "        #<a href=\"#search-autocomplete\" class=>Skip to search</a>\n",
    "        name=containers.find('div', attrs={'class':'img-wrap'})\n",
    "        \n",
    "        price=containers.find('div', attrs={'class':'js-tracked-promo dotcms-include brand-tiles'})\n",
    "        \n",
    "        rating=containers.find('nav', attrs={'class':'sfw-header_wrapper chirp-sfw'})\n",
    "        \n",
    "        specification = containers.find('div', attrs={'class':'cms-include'})\n",
    "        \n",
    "        ## Splitting integrated specification into individual CPU, RAM, OS, HDD and Display specifications\n",
    "        for col in specification:\n",
    "            col=col.find_all('a', attrs={'class':'cw-desktop'})\n",
    "            ProductName_Description = col[0].text                 #List to store ProductName_Description sspecifications of the product\n",
    "            Size =col[1].text                 #List to store ssize specifications of the product\n",
    "            Ingredients=col[2].text                  #List to store Ingredients specifications of the product\n",
    "            Images = col[3].text                  #List to store images specifications of the product\n",
    "            Categories =col[4].text              #List to store Categories specifications of the product\n",
    "            Key_Benefits = col[5].text          #List to storeKey Benefits specifications of the product     \n",
    "            Brand  = col[6].text               #List to brand specifications of the product \n",
    "            price = col[7].text   \n",
    "        \n",
    "        products.append(Name.text) # Add product name to list\n",
    "        \n",
    "        prices.append(price.text) # Add price to list\n",
    "        \n",
    "        #specifications.append(specification.text) if type(specification) == bs4.element.Tag  else specifications.append('NaN')\n",
    "        \n",
    "        Prd,append(pde) # Add ProductName_Description specifications to list\n",
    "        \n",
    "        si.append(siz) # Add Size specifications to list\n",
    "        \n",
    "        ind.append(ing) # Add ingredients specifications to list\n",
    "        \n",
    "        im.append(img) # Add Images specifications to list\n",
    "        \n",
    "        cate.append(cte) # Add Categories to list\n",
    "        \n",
    "        kb.append(keb) # Add Key Benefits specifications to list\n",
    "        \n",
    "        br.append(brn) # Add brand specifications to list\n",
    "        \n",
    "        pr.append(pcr) # Add price specifications to list\n",
    "        \n",
    "        rat.append(rating.text) if type(rating) == bs4.element.Tag  else ratings.append('NaN') # Add Rating to list\n",
    "        \n",
    "        count=count+1 # Increment row count\n",
    "    \n",
    "    ## Create a dataframe with structured data from all searched rows\n",
    "    df = pd.DataFrame({'product_name':products,'ProductName_Description':prd,'Size':si,'Ingredients':im,'Categories':cate,'Key Benefits':kb,'Brand':br,'Price':pr})\n",
    "\n",
    "print('No. of rows searched',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
